{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Recognition\n",
    "\n",
    "In this workbook I practice:\n",
    "* Basic machine learning models including K-means, K-NN, SVM, and Adaboost using scit-learn\n",
    "* Bag-of-Words model\n",
    "* Applying machine learning models for image recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `./resources/FoodImages`, there are two folders: Train and Test containing training and test images respectively.\n",
    "Each Train/Test folder contains three sub-folders corresponding to three different food types including `Cakes`, `Pasta`,\n",
    "and `Pizza`. \n",
    "\n",
    "For each food category, there are equal numbers of images (30 images) used for training and testing. I build a BoW model for food image recognition based on the training images of the supplied food database.\n",
    "\n",
    "The Dictionary class below is developed to build BoW models using K-means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self, name, img_filenames, num_words):\n",
    "        self.name = name #name of your dictionary\n",
    "        self.img_filenames = img_filenames #list of image filenames\n",
    "        self.num_words = num_words #the number of words\n",
    "\n",
    "        self.training_data = [] #this is the training data required by the K-Means algorithm\n",
    "        self.words = [] #list of words, which are the centroids of clusters\n",
    "\n",
    "    def learn(self):\n",
    "        sift = cv.xfeatures2d.SIFT_create()\n",
    "\n",
    "        num_keypoints = [] #this is used to store the number of keypoints in each image\n",
    "\n",
    "        #load training images and compute SIFT descriptors\n",
    "        for filename in self.img_filenames:\n",
    "            img = cv.imread(filename)\n",
    "            img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "            list_des = sift.detectAndCompute(img_gray, None)[1]\n",
    "            if list_des is None:\n",
    "                num_keypoints.append(0)\n",
    "            else:\n",
    "                num_keypoints.append(len(list_des))\n",
    "                for des in list_des:\n",
    "                    self.training_data.append(des)\n",
    "\n",
    "        #cluster SIFT descriptors using K-means algorithm\n",
    "        kmeans = KMeans(self.num_words)\n",
    "        kmeans.fit(self.training_data)\n",
    "        self.words = kmeans.cluster_centers_\n",
    "\n",
    "        #create word histograms for training images\n",
    "        training_word_histograms = [] #list of word histograms of all training images\n",
    "        index = 0\n",
    "        for i in range(0, len(self.img_filenames)):\n",
    "            #for each file, create a histogram\n",
    "            histogram = np.zeros(self.num_words, np.float32)\n",
    "            #if some keypoints exist\n",
    "            if num_keypoints[i] > 0:\n",
    "                for j in range(0, num_keypoints[i]):\n",
    "                    histogram[kmeans.labels_[j + index]] += 1\n",
    "                index += num_keypoints[i]\n",
    "                histogram /= num_keypoints[i]\n",
    "                training_word_histograms.append(histogram)\n",
    "\n",
    "        return training_word_histograms\n",
    "\n",
    "    def create_word_histograms(self, img_filenames):\n",
    "        sift = cv.xfeatures2d.SIFT_create()\n",
    "        histograms = []\n",
    "\n",
    "        for filename in img_filenames:\n",
    "            img = cv.imread(filename)\n",
    "            img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "            descriptors = sift.detectAndCompute(img_gray, None)[1]\n",
    "\n",
    "            histogram = np.zeros(self.num_words, np.float32) #word histogram for the input image\n",
    "\n",
    "            if descriptors is not None:\n",
    "                for des in descriptors:\n",
    "                    #find the best matching word\n",
    "                    min_distance = 1111111 #this can be any large number\n",
    "                    matching_word_ID = -1 #initial matching_word_ID=-1 means no matching\n",
    "\n",
    "                    for i in range(0, self.num_words): #search for the best matching word\n",
    "                        distance = np.linalg.norm(des - self.words[i])\n",
    "                        if distance < min_distance:\n",
    "                            min_distance = distance\n",
    "                            matching_word_ID = i\n",
    "\n",
    "                    histogram[matching_word_ID] += 1\n",
    "\n",
    "                histogram /= len(descriptors) #normalise histogram to frequencies\n",
    "\n",
    "            histograms.append(histogram)\n",
    "\n",
    "        return histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a list of train and test file names to pass into the dictionary class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "foods = ['Cakes', 'Pasta', 'Pizza']\n",
    "path = './resources/FoodImages/'\n",
    "\n",
    "training_file_names = []\n",
    "training_food_labels = []\n",
    "\n",
    "test_file_names = []\n",
    "test_food_labels = []\n",
    "\n",
    "for i in range(0, len(foods)):\n",
    "    sub_path = path + 'Train/' + foods[i] + '/'\n",
    "    sub_file_names = [os.path.join(sub_path, f) for f in os.listdir(sub_path)]\n",
    "    sub_food_labels = [i] * len(sub_file_names)\n",
    "    training_file_names += sub_file_names\n",
    "    training_food_labels += sub_food_labels\n",
    "    \n",
    "for i in range(0, len(foods)):\n",
    "    sub_path = path + 'Test/' + foods[i] + '/'\n",
    "    sub_file_names = [os.path.join(sub_path, f) for f in os.listdir(sub_path)]\n",
    "    sub_food_labels = [i] * len(sub_file_names)\n",
    "    test_file_names += sub_file_names\n",
    "    test_food_labels += sub_food_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of the Dictionary class and then train the BoW model and extract the word historgrams for teh train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 50\n",
    "dictionary_name = 'food'\n",
    "dictionary = Dictionary(dictionary_name, training_file_names, num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_word_histograms = dictionary.learn()\n",
    "test_word_histograms = dictionary.create_word_histograms(test_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('food_dictionary.dic', 'wb') as f:\n",
    "    pickle.dump(dictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('food_dictionary.dic', 'rb') as f: #'rb' is for binary read\n",
    "    dictionary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks =  [5, 10, 15, 20, 25, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for k=5: 73.33%\n",
      "confusion matrix:\n",
      "[[20  4  6]\n",
      " [ 0 27  3]\n",
      " [ 1 10 19]]\n",
      "--------------------------------------------\n",
      "\n",
      "accuracy for k=10: 70.00%\n",
      "confusion matrix:\n",
      "[[14  7  9]\n",
      " [ 0 27  3]\n",
      " [ 0  8 22]]\n",
      "--------------------------------------------\n",
      "\n",
      "accuracy for k=15: 66.67%\n",
      "confusion matrix:\n",
      "[[13  7 10]\n",
      " [ 0 27  3]\n",
      " [ 0 10 20]]\n",
      "--------------------------------------------\n",
      "\n",
      "accuracy for k=20: 64.44%\n",
      "confusion matrix:\n",
      "[[11  9 10]\n",
      " [ 0 26  4]\n",
      " [ 0  9 21]]\n",
      "--------------------------------------------\n",
      "\n",
      "accuracy for k=25: 62.22%\n",
      "confusion matrix:\n",
      "[[ 9 10 11]\n",
      " [ 0 27  3]\n",
      " [ 0 10 20]]\n",
      "--------------------------------------------\n",
      "\n",
      "accuracy for k=30: 60.00%\n",
      "confusion matrix:\n",
      "[[ 8 11 11]\n",
      " [ 0 27  3]\n",
      " [ 0 11 19]]\n",
      "--------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in ks:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(training_word_histograms, training_food_labels)\n",
    "    predicted_food_labels = knn.predict(test_word_histograms)\n",
    "    print(f\"\"\"accuracy for k={k}: {accuracy_score(test_food_labels, predicted_food_labels)*100:.2f}%\n",
    "confusion matrix:\n",
    "{confusion_matrix(test_food_labels, predicted_food_labels)}\n",
    "--------------------------------------------\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = [10, 20, 30, 40, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for C=10: 78.89%\n",
      "confusion matrix:\n",
      "[[25  3  2]\n",
      " [ 0 25  5]\n",
      " [ 1  8 21]]\n",
      "--------------------------------------------\n",
      "\n",
      "accuracy for C=20: 82.22%\n",
      "confusion matrix:\n",
      "[[26  2  2]\n",
      " [ 0 24  6]\n",
      " [ 1  5 24]]\n",
      "--------------------------------------------\n",
      "\n",
      "accuracy for C=30: 82.22%\n",
      "confusion matrix:\n",
      "[[26  2  2]\n",
      " [ 0 24  6]\n",
      " [ 1  5 24]]\n",
      "--------------------------------------------\n",
      "\n",
      "accuracy for C=40: 84.44%\n",
      "confusion matrix:\n",
      "[[27  1  2]\n",
      " [ 0 24  6]\n",
      " [ 1  4 25]]\n",
      "--------------------------------------------\n",
      "\n",
      "accuracy for C=50: 85.56%\n",
      "confusion matrix:\n",
      "[[28  1  1]\n",
      " [ 0 24  6]\n",
      " [ 1  4 25]]\n",
      "--------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in cs:\n",
    "    svm_classifier = svm.SVC(C = c,\n",
    "                         kernel = 'linear')\n",
    "    svm_classifier.fit(training_word_histograms, training_food_labels)\n",
    "    predicted_food_labels = svm_classifier.predict(test_word_histograms)\n",
    "    print(f\"\"\"accuracy for C={c}: {accuracy_score(test_food_labels, predicted_food_labels)*100:.2f}%\n",
    "confusion matrix:\n",
    "{confusion_matrix(test_food_labels, predicted_food_labels)}\n",
    "--------------------------------------------\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estmators =  [50, 100, 150, 200, 250, 300, 400, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for n=50: 65.56%\n",
      "confusion matrix:\n",
      "[[17  2 11]\n",
      " [ 1 17 12]\n",
      " [ 3  2 25]]\n",
      "--------------------------------------------\n",
      "\n",
      "accuracy for n=100: 61.11%\n",
      "confusion matrix:\n",
      "[[17  3 10]\n",
      " [ 1 16 13]\n",
      " [ 3  5 22]]\n",
      "--------------------------------------------\n",
      "\n",
      "accuracy for n=150: 68.89%\n",
      "confusion matrix:\n",
      "[[21  3  6]\n",
      " [ 1 20  9]\n",
      " [ 2  7 21]]\n",
      "--------------------------------------------\n",
      "\n",
      "accuracy for n=200: 75.56%\n",
      "confusion matrix:\n",
      "[[21  3  6]\n",
      " [ 0 23  7]\n",
      " [ 0  6 24]]\n",
      "--------------------------------------------\n",
      "\n",
      "accuracy for n=250: 76.67%\n",
      "confusion matrix:\n",
      "[[20  3  7]\n",
      " [ 0 22  8]\n",
      " [ 0  3 27]]\n",
      "--------------------------------------------\n",
      "\n",
      "accuracy for n=300: 74.44%\n",
      "confusion matrix:\n",
      "[[20  4  6]\n",
      " [ 0 23  7]\n",
      " [ 0  6 24]]\n",
      "--------------------------------------------\n",
      "\n",
      "accuracy for n=400: 75.56%\n",
      "confusion matrix:\n",
      "[[21  3  6]\n",
      " [ 0 22  8]\n",
      " [ 0  5 25]]\n",
      "--------------------------------------------\n",
      "\n",
      "accuracy for n=500: 77.78%\n",
      "confusion matrix:\n",
      "[[21  4  5]\n",
      " [ 0 22  8]\n",
      " [ 0  3 27]]\n",
      "--------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in n_estmators:\n",
    "    adb_classifier = AdaBoostClassifier(n_estimators = n,\n",
    "                                        random_state = 0)\n",
    "\n",
    "    adb_classifier.fit(training_word_histograms, training_food_labels)\n",
    "    predicted_food_labels = adb_classifier.predict(test_word_histograms)\n",
    "    print(f\"\"\"accuracy for n={n}: {accuracy_score(test_food_labels, predicted_food_labels)*100:.2f}%\n",
    "confusion matrix:\n",
    "{confusion_matrix(test_food_labels, predicted_food_labels)}\n",
    "--------------------------------------------\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Upon testing 3 different algorithms with various parameters. The best model parameters for each algorithm was:\n",
    "* k-NN: k=5\n",
    "* SVM: C=40 & 50 (2-way tie)\n",
    "* AdaBoost: n_estimators = 150, 200, 300 (3-way tie)\n",
    "\n",
    "Overall, the best algorithm was the SVM achieving the highest test set accuracy of 85.56%. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
